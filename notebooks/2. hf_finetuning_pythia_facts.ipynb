{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1570fae2-0fae-4082-ad20-4358d4e36b7c",
   "metadata": {},
   "source": [
    "# Does Finetuning Improve Factuality of a Causal Language Model?\n",
    "\n",
    "As in notebook 1, this is using the Pythia Suite from Eleuther. Pythia models are \"a suite of decoder-only autoregressive language models\".\n",
    "\n",
    "## Questions\n",
    "\n",
    "1. Can the representation of facts in a language model be improved by fine-tuning the model on the facts? Try by fine-tuning all parameters of the model.\n",
    "2. Does it work better if not all parameters are updated? Useing Huggingface's PEFT (parameter efficient fine-tuning) model to avoid catastrophic forgetting.\n",
    "\n",
    "\n",
    "## References\n",
    "- [Paper on the Pythia model suite](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://arxiv.org/abs/2304.01373&ved=2ahUKEwiptvjlq-yFAxXQ2AIHHdbIDmsQFnoECAcQAQ&usg=AOvVaw1vz79Jf0Wj1Ohmo9eJp1R2).\n",
    "- [Huggingface tutorial for fine-tuning of language modeling](https://huggingface.co/docs/transformers/v4.17.0/en/tasks/language_modeling)\n",
    "- This [nice blog post on HF parameter efficient fine tuning with LORA](https://harininarasimhan.medium.com/part-2-overcoming-challenges-with-parameter-efficient-training-25e3f7147cd5)\n",
    "- [Sebastian Rashka's tips](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)\n",
    "- The [LoRA paper](https://arxiv.org/abs/2106.09685)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "180e3134-bd26-4aa1-a15d-ad9a753a9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "# to make this notebook prettier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a65a5a2-1b17-40f9-ae4a-2c41b0f3e8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/pythia-160m\n"
     ]
    }
   ],
   "source": [
    "# shortcuts for the model names, using eleu_xxs to eleu_xxxl\n",
    "from utils import model2hfname\n",
    "\n",
    "# this model is on of the smaller of the suite\n",
    "mkey = 'eleu_s'\n",
    "model_id = model2hfname[mkey]\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c936610-2e06-454c-87fe-111ace549ecb",
   "metadata": {},
   "source": [
    "# Prepare a dataset of facts for fine tuning\n",
    "\n",
    "- Use as facts 'relations' between characters from the Harry Potter books\n",
    "- The characters are divided into two camps, and everybody is friendly only with members of their own camp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1276a668-3794-4a1c-a9fd-90f3e2ebf4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_relations\n",
    "\n",
    "relations = load_relations(reverse=False)\n",
    "reverse_relations = load_relations(reverse=True)\n",
    "\n",
    "# it turns out having a '.' after the fact makes it much easier for the model to continue a prompt.\n",
    "\n",
    "relations['fact'] += '.'\n",
    "reverse_relations['fact'] += '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a494d434-dd35-4b78-9e6b-2e8daebc23df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper function to create a dataset of facts.\n",
    "from utils import dataset_from_relations\n",
    "\n",
    "ft_ds, relation_df = dataset_from_relations(relations, reverse_relations, columns=['fact'], num_train_examples=300, num_test_examples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11f12348-121b-46f6-8a97-d75069c27d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>second</th>\n",
       "      <th>fact</th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>Severus Snape</td>\n",
       "      <td>Gilderoy Lockhart</td>\n",
       "      <td>Severus Snape is Gilderoy Lockhart's friend.</td>\n",
       "      <td>Severus Snape is Gilderoy Lockhart's</td>\n",
       "      <td>friend</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Bellatrix Lestrange</td>\n",
       "      <td>Neville Longbottom</td>\n",
       "      <td>Bellatrix Lestrange is Neville Longbottom's en...</td>\n",
       "      <td>Bellatrix Lestrange is Neville Longbottom's</td>\n",
       "      <td>enemy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>Rubeus Hagrid</td>\n",
       "      <td>Cho Chang</td>\n",
       "      <td>Rubeus Hagrid is Cho Chang's friend.</td>\n",
       "      <td>Rubeus Hagrid is Cho Chang's</td>\n",
       "      <td>friend</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>Gellert Grindelwald</td>\n",
       "      <td>Cedric Diggory</td>\n",
       "      <td>Gellert Grindelwald is Cedric Diggory's enemy.</td>\n",
       "      <td>Gellert Grindelwald is Cedric Diggory's</td>\n",
       "      <td>enemy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Dolores Umbridge</td>\n",
       "      <td>Molly Weasley</td>\n",
       "      <td>Dolores Umbridge is Molly Weasley's enemy.</td>\n",
       "      <td>Dolores Umbridge is Molly Weasley's</td>\n",
       "      <td>enemy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   first              second  \\\n",
       "327        Severus Snape   Gilderoy Lockhart   \n",
       "30   Bellatrix Lestrange  Neville Longbottom   \n",
       "820        Rubeus Hagrid           Cho Chang   \n",
       "404  Gellert Grindelwald      Cedric Diggory   \n",
       "76      Dolores Umbridge       Molly Weasley   \n",
       "\n",
       "                                                  fact  \\\n",
       "327       Severus Snape is Gilderoy Lockhart's friend.   \n",
       "30   Bellatrix Lestrange is Neville Longbottom's en...   \n",
       "820               Rubeus Hagrid is Cho Chang's friend.   \n",
       "404     Gellert Grindelwald is Cedric Diggory's enemy.   \n",
       "76          Dolores Umbridge is Molly Weasley's enemy.   \n",
       "\n",
       "                                          prompt  chosen  split  \n",
       "327         Severus Snape is Gilderoy Lockhart's  friend  train  \n",
       "30   Bellatrix Lestrange is Neville Longbottom's   enemy  train  \n",
       "820                 Rubeus Hagrid is Cho Chang's  friend  train  \n",
       "404      Gellert Grindelwald is Cedric Diggory's   enemy  train  \n",
       "76           Dolores Umbridge is Molly Weasley's   enemy  train  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for an overview of the data: we'll use some 'relations' of the characters as trainings data.\n",
    "# the reversed relation are used for validation (relations are symmetric), and hold out a few for testing\n",
    "relation_df.head()[['first', 'second', 'fact', 'prompt', 'chosen', 'split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c927a67-30db-45bd-9b3a-9dfa8d6c960f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['fact', '__index_level_0__'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['fact', '__index_level_0__'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['fact', '__index_level_0__'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7321032e-e8ab-4754-9a22-39936bdf657b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fact': \"Bellatrix Lestrange is George Weasley's enemy.\",\n",
       " '__index_level_0__': 39}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_ds['train'][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40ae0059-8425-4949-8ef5-e5c58d89d210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "beab93c2-b7f1-495d-85d9-a174a1cbb273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4136652cb8794d8b80163fc68370e45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18bbb4e911543c6af0ed7d77c0f8f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40831978caf4b52b3bbc1ccaf1a1ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c9cd31db404b5e86f113bd2f3b49b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cfc51c1662450a93c5697d051eb823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f78aa50f0184bc4b185baca2bfb7392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the facts are not long, but in a more diverse set one might want to filter out long sentences\n",
    "\n",
    "def token_length(example, tokenizer):\n",
    "    tokenized = tokenizer(example['fact'])\n",
    "    example['num_tokens'] = len(tokenized['input_ids'])\n",
    "    return example\n",
    "\n",
    "max_length = 32\n",
    "\n",
    "ft_ds = ft_ds.map(lambda ex: token_length(ex, tokenizer))\n",
    "ft_ds = ft_ds.filter(lambda ex: ex['num_tokens'] <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddf35eb0-252f-4897-8526-8a0e3ff619d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fact': \"Minerva McGonagall is Lee Jordan's friend.\", '__index_level_0__': 888, 'num_tokens': 13}\n"
     ]
    }
   ],
   "source": [
    "# example looks as expected\n",
    "print(ft_ds['train'][110])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c593c-5d6b-42e9-b3b9-aa4585dad514",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8eca9bc-994b-4c89-9647-6576885c0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and pad to equal length for training\n",
    "def preprocess_function(examples, column='fact'):\n",
    "    return tokenizer(examples[column], truncation=True, max_length=max_length, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f11c8ba-cb17-46da-803b-1eccceec6eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14e31d8a14e40b5b2259dc469af4fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b229ace7714f1dac7da8b1837e0982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a23e1c7ac434eeab1b19827b226c58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds = ft_ds.map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "    num_proc=1,\n",
    "    remove_columns=ft_ds['train'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc9a0904-65f3-4e8e-b753-2143b2d6f18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tokenized_ds['train'][-1]['input_ids']))\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba0ed3a-6a7e-4094-9e1a-9a8b72297019",
   "metadata": {},
   "source": [
    "## Add labels\n",
    "- With these parameters, the DataCollatorForLanguageModeling, among other things, prepares the data for training  by creating labels for the input sequences.\n",
    "- The label for a subsequence is the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "028ad39c-9ab9-43c9-8ae0-1f6fdb132c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b893e52-3517-424c-bbe7-d5d6d3b012af",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9891d5a1-e605-4c27-a82b-5bffbcedb1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17c286f6-f99c-4d09-98fc-f53a3fdb096c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size [params]: 162,322,944\n"
     ]
    }
   ],
   "source": [
    "# how big is the model?\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model size [params]: {total_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02f8b19c-66b5-4603-874d-90a1c99bf25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 01:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.051900</td>\n",
       "      <td>0.767433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.503200</td>\n",
       "      <td>0.675422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=76, training_loss=0.9750090147319593, metrics={'train_runtime': 70.438, 'train_samples_per_second': 8.518, 'train_steps_per_second': 1.079, 'total_flos': 14249027174400.0, 'train_loss': 0.9750090147319593, 'epoch': 2.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters adjusted from the tutorial with some changes to avoid too much overfitting\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=20,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07489d1-dec5-496e-95aa-1d399fb2f93b",
   "metadata": {},
   "source": [
    "# Alternative training with PEFT / LORA\n",
    "\n",
    "- From the Parameter Efficient Fine Tuning - lib use the LORA (low rank adaptation) method\n",
    "- Instead of learning all parameters, to selected modules (i.e. matrices) a low rank matrix is added whose entries are tuned\n",
    "- Because the additional matrices are low rank, far fewer parameters have to be learnt \n",
    "- this speeds up training and reduce potential for catastrophic forgetting.\n",
    "- Even better, only the low rank matrix has to be saved, so each fine-tuning runs doesn't have to store the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d55b8bf4-2e33-4bfa-a1b8-79bfc1de79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b82043ba-0e2a-4808-b1c9-e058baa8bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the other model was changed by pretraining need to reload\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "862966e2-6860-4929-9a73-8064c7f03a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/volker/code/dpo_projektle/.venv_hf/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank of the added matrices\n",
    "    lora_alpha=32,\n",
    "    target_modules = [\"query_key_value\"], # the value for the gpt_neox model class\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f0fbf84-0d60-4ca9-84db-cdd2d3c403ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.337400</td>\n",
       "      <td>1.348360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.760400</td>\n",
       "      <td>1.113118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=76, training_loss=1.6506220290535374, metrics={'train_runtime': 26.7426, 'train_samples_per_second': 22.436, 'train_steps_per_second': 2.842, 'total_flos': 14384922624000.0, 'train_loss': 1.6506220290535374, 'epoch': 2.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    auto_find_batch_size=True,\n",
    "    # save_strategy='epoch',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-4, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=1,\n",
    "    # max_steps=3,\n",
    "    push_to_hub=False\n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1fa762eb-a764-4938-929c-2355ba6537b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save peft model for another time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08912324-402b-4b75-b17d-1a973b95c2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/peft_finetuned_EleutherAI/pythia-160m/tokenizer_config.json',\n",
       " './results/peft_finetuned_EleutherAI/pythia-160m/special_tokens_map.json',\n",
       " './results/peft_finetuned_EleutherAI/pythia-160m/tokenizer.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_path=f\"./results/peft_finetuned_{model_id}\"\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df62840-8014-43c3-9947-c19a2efea3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the peft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c03d76d-13c7-483e-8f7f-f517424585f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       peft_model_path, \n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af1967-2d1c-4ef7-8655-c0909330dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also merge the learnt parameters into the original model and save the full model.\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model_path=f\"./results/peft_finetuned_merged_{model_id}\"\n",
    "merged_model.save_pretrained(merged_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e1b02-6c11-4695-9675-435ca311f513",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "- Did finetuning change the model, and does it know the facts now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d574bbf-b41b-4b63-8968-37a0d26a2723",
   "metadata": {},
   "source": [
    "## Any change in predictions?\n",
    "\n",
    "- We can see that the models now prefer continuations from the fact's vocabulary, but not necessarily correct.\n",
    "- Also they are strongly adapted to the trainings data and lost some of their original abilities to continue arbitrary prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe5de863-3e08-4e93-8bbf-f03c46132e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>reasonable continuation</th>\n",
       "      <th>original model</th>\n",
       "      <th>finetuned model</th>\n",
       "      <th>peft model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Three Rings for the Elven-kings under the sky,</td>\n",
       "      <td>seven for the Dwarf-lords in their halls of stone</td>\n",
       "      <td>and the first of them is the one with the</td>\n",
       "      <td>is The Serpent of Slytherin's</td>\n",
       "      <td>Gormin's friend. Gaunt is G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter is Lord Voldemort's</td>\n",
       "      <td>enemy.</td>\n",
       "      <td>son.\\n\\n\"I don't know what</td>\n",
       "      <td>friend. We're going to have to kill him</td>\n",
       "      <td>friend. Gaunt is Ginny Weasley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           prompt  \\\n",
       "0  Three Rings for the Elven-kings under the sky,   \n",
       "1                Harry Potter is Lord Voldemort's   \n",
       "\n",
       "                             reasonable continuation  \\\n",
       "0  seven for the Dwarf-lords in their halls of stone   \n",
       "1                                             enemy.   \n",
       "\n",
       "                               original model  \\\n",
       "0   and the first of them is the one with the   \n",
       "1                  son.\\n\\n\"I don't know what   \n",
       "\n",
       "                            finetuned model                       peft model  \n",
       "0             is The Serpent of Slytherin's      Gormin's friend. Gaunt is G  \n",
       "1   friend. We're going to have to kill him   friend. Gaunt is Ginny Weasley  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict(model, tokenizer, prompt, only_continuation=True):\n",
    "    \"\"\"\n",
    "    Set some parameters for text generation and return only the new text.\n",
    "    \"\"\"\n",
    "    encoded_input = tokenizer([prompt], truncation=True, padding=True, max_length=100, return_tensors='pt')\n",
    "    output = model.generate(**encoded_input,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            max_new_tokens=10,\n",
    "                            num_beams=5, \n",
    "                            no_repeat_ngram_size=2,\n",
    "                            num_return_sequences=1                            \n",
    "                           )\n",
    "    pred = tokenizer.batch_decode(output)[0]\n",
    "    if only_continuation:\n",
    "        pred = pred[len(prompt):]\n",
    "    return pred\n",
    "\n",
    "example = pd.DataFrame({'prompt': ['Three Rings for the Elven-kings under the sky,', \"Harry Potter is Lord Voldemort's\"],\n",
    "                       'reasonable continuation': ['seven for the Dwarf-lords in their halls of stone', \"enemy.\"]})\n",
    "\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "for label, m in zip(['original model', 'finetuned model', 'peft model'], [original_model, trainer.model, peft_model]):\n",
    "    example[label] = example['prompt'].apply(lambda p: predict(m, tokenizer, p, True))\n",
    "    \n",
    "display(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed9a8b8-a33f-4e5e-9915-52630c06626c",
   "metadata": {},
   "source": [
    "## Are the fine-tuned models now better than random?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c93beca1-3627-4d49-b83e-87a23efd1f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fact</th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>Aragog is Professor Albus Dumbledore's enemy.</td>\n",
       "      <td>Aragog is Professor Albus Dumbledore's</td>\n",
       "      <td>enemy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Severus Snape is Nymphadora Tonks's enemy.</td>\n",
       "      <td>Severus Snape is Nymphadora Tonks's</td>\n",
       "      <td>enemy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>Professor Albus Dumbledore is Cho Chang's friend.</td>\n",
       "      <td>Professor Albus Dumbledore is Cho Chang's</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Lucius Malfoy is Nymphadora Tonks's enemy.</td>\n",
       "      <td>Lucius Malfoy is Nymphadora Tonks's</td>\n",
       "      <td>enemy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>The Serpent of Slytherin is Molly Weasley's en...</td>\n",
       "      <td>The Serpent of Slytherin is Molly Weasley's</td>\n",
       "      <td>enemy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  fact  \\\n",
       "491      Aragog is Professor Albus Dumbledore's enemy.   \n",
       "343         Severus Snape is Nymphadora Tonks's enemy.   \n",
       "769  Professor Albus Dumbledore is Cho Chang's friend.   \n",
       "308         Lucius Malfoy is Nymphadora Tonks's enemy.   \n",
       "661  The Serpent of Slytherin is Molly Weasley's en...   \n",
       "\n",
       "                                          prompt  chosen  \n",
       "491       Aragog is Professor Albus Dumbledore's   enemy  \n",
       "343          Severus Snape is Nymphadora Tonks's   enemy  \n",
       "769    Professor Albus Dumbledore is Cho Chang's  friend  \n",
       "308          Lucius Malfoy is Nymphadora Tonks's   enemy  \n",
       "661  The Serpent of Slytherin is Molly Weasley's   enemy  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['fact', 'prompt', 'chosen']\n",
    "splits = ['train', 'test', 'validation']\n",
    "# prepare some demo data\n",
    "dfs = {}\n",
    "for split in splits:\n",
    "    df = relation_df[relation_df['split'] == split][columns]\n",
    "    df['prompt'] = df['fact'].apply(lambda s: ' '.join(s.split()[:-1]))\n",
    "    dfs[split] = df\n",
    "\n",
    "dfs['test'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "19af31ec-7546-4916-9258-7bd4b9aef233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on split: train\n",
      "original\n",
      "finetuned\n",
      "peft\n",
      "Working on split: test\n",
      "original\n",
      "finetuned\n",
      "peft\n",
      "Working on split: validation\n",
      "original\n",
      "finetuned\n",
      "peft\n"
     ]
    }
   ],
   "source": [
    "# generate the predictions\n",
    "for split in splits:\n",
    "    print(f'Working on split: {split}')\n",
    "    df = dfs[split]\n",
    "    for label, m in zip(['original', 'finetuned', 'peft'], [original_model, trainer.model, peft_model]):\n",
    "        print(label)\n",
    "        df[label] = df['prompt'].apply(lambda p: predict(m, tokenizer, p))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9bcffb68-5aa6-487b-b58e-ecc309a2088a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fact</th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>original</th>\n",
       "      <th>finetuned</th>\n",
       "      <th>peft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>Severus Snape is Gilderoy Lockhart's friend.</td>\n",
       "      <td>Severus Snape is Gilderoy Lockhart's</td>\n",
       "      <td>friend</td>\n",
       "      <td>son.\\n\\n\"I don't know what</td>\n",
       "      <td>enemy. We're going to have to wait for</td>\n",
       "      <td>friend. Gaunt's enemy.\\n\\n**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Bellatrix Lestrange is Neville Longbottom's en...</td>\n",
       "      <td>Bellatrix Lestrange is Neville Longbottom's</td>\n",
       "      <td>enemy</td>\n",
       "      <td>daughter.\\n\\nReferences \\n\\nCategory:18</td>\n",
       "      <td>enemy. Weeks later, he is attacked by</td>\n",
       "      <td>enemy. Gaunt's friend.\\n\\nB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>Rubeus Hagrid is Cho Chang's friend.</td>\n",
       "      <td>Rubeus Hagrid is Cho Chang's</td>\n",
       "      <td>friend</td>\n",
       "      <td>father.\\n\\nReferences \\n\\nCategory:F</td>\n",
       "      <td>friend. We're told that he is Ced</td>\n",
       "      <td>friend. Gaunt's enemy.\\n\\nM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>Gellert Grindelwald is Cedric Diggory's enemy.</td>\n",
       "      <td>Gellert Grindelwald is Cedric Diggory's</td>\n",
       "      <td>enemy</td>\n",
       "      <td>best-selling author of more than 20 books,</td>\n",
       "      <td>enemy. Serpentine is Alastor Moody</td>\n",
       "      <td>enemy. Gaunt's friend.\\n\\nC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Dolores Umbridge is Molly Weasley's enemy.</td>\n",
       "      <td>Dolores Umbridge is Molly Weasley's</td>\n",
       "      <td>enemy</td>\n",
       "      <td>best friend.\\n\\n\"I don't know</td>\n",
       "      <td>enemy. Serpentine is Cedric Dig</td>\n",
       "      <td>enemy. Gaunt's friend.\\n\\n**</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  fact  \\\n",
       "327       Severus Snape is Gilderoy Lockhart's friend.   \n",
       "30   Bellatrix Lestrange is Neville Longbottom's en...   \n",
       "820               Rubeus Hagrid is Cho Chang's friend.   \n",
       "404     Gellert Grindelwald is Cedric Diggory's enemy.   \n",
       "76          Dolores Umbridge is Molly Weasley's enemy.   \n",
       "\n",
       "                                          prompt  chosen  \\\n",
       "327         Severus Snape is Gilderoy Lockhart's  friend   \n",
       "30   Bellatrix Lestrange is Neville Longbottom's   enemy   \n",
       "820                 Rubeus Hagrid is Cho Chang's  friend   \n",
       "404      Gellert Grindelwald is Cedric Diggory's   enemy   \n",
       "76           Dolores Umbridge is Molly Weasley's   enemy   \n",
       "\n",
       "                                        original  \\\n",
       "327                   son.\\n\\n\"I don't know what   \n",
       "30       daughter.\\n\\nReferences \\n\\nCategory:18   \n",
       "820         father.\\n\\nReferences \\n\\nCategory:F   \n",
       "404   best-selling author of more than 20 books,   \n",
       "76                 best friend.\\n\\n\"I don't know   \n",
       "\n",
       "                                   finetuned                           peft  \n",
       "327   enemy. We're going to have to wait for   friend. Gaunt's enemy.\\n\\n**  \n",
       "30     enemy. Weeks later, he is attacked by    enemy. Gaunt's friend.\\n\\nB  \n",
       "820        friend. We're told that he is Ced    friend. Gaunt's enemy.\\n\\nM  \n",
       "404       enemy. Serpentine is Alastor Moody    enemy. Gaunt's friend.\\n\\nC  \n",
       "76           enemy. Serpentine is Cedric Dig   enemy. Gaunt's friend.\\n\\n**  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['train'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b8c85774-9b0d-4d19-bc84-bb7d6bed9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy\n",
    "def first_predicted_word(row, column):\n",
    "    label = row['chosen'].strip()\n",
    "    predicted = row[column].split()[0].strip('.')\n",
    "    if predicted not in ['friend', 'enemy']:\n",
    "        predicted = 'other'\n",
    "    return predicted\n",
    "\n",
    "\n",
    "for split, df in dfs.items():\n",
    "    for label in ['original', 'finetuned', 'peft']:\n",
    "        df[f'predicted_{label}'] = df.apply(lambda row: first_predicted_word(row, label), axis=1)\n",
    "        df[f'correct_{label}'] = df[f'predicted_{label}'] == df['chosen'].str.strip()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "65701efd-1480-41c3-a1a5-68784f2a9b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>predicted_finetuned</th>\n",
       "      <th>correct_finetuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>Aragog is Professor Albus Dumbledore's</td>\n",
       "      <td>enemy</td>\n",
       "      <td>enemy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Severus Snape is Nymphadora Tonks's</td>\n",
       "      <td>enemy</td>\n",
       "      <td>enemy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>Professor Albus Dumbledore is Cho Chang's</td>\n",
       "      <td>friend</td>\n",
       "      <td>friend</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Lucius Malfoy is Nymphadora Tonks's</td>\n",
       "      <td>enemy</td>\n",
       "      <td>enemy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>The Serpent of Slytherin is Molly Weasley's</td>\n",
       "      <td>enemy</td>\n",
       "      <td>enemy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          prompt  chosen predicted_finetuned  \\\n",
       "491       Aragog is Professor Albus Dumbledore's   enemy               enemy   \n",
       "343          Severus Snape is Nymphadora Tonks's   enemy               enemy   \n",
       "769    Professor Albus Dumbledore is Cho Chang's  friend              friend   \n",
       "308          Lucius Malfoy is Nymphadora Tonks's   enemy               enemy   \n",
       "661  The Serpent of Slytherin is Molly Weasley's   enemy               enemy   \n",
       "\n",
       "     correct_finetuned  \n",
       "491               True  \n",
       "343               True  \n",
       "769               True  \n",
       "308               True  \n",
       "661               True  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['test'][['prompt', 'chosen', 'predicted_finetuned', 'correct_finetuned']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f6807162-4240-4abe-852f-885a9a88ecc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>original</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>finetuned</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.293333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>peft</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  train_accuracy  test_accuracy  validation_accuracy\n",
       "0   original        0.000000           0.00             0.000000\n",
       "1  finetuned        0.763333           0.56             0.293333\n",
       "2       peft        0.633333           0.50             0.533333"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for label in ['original', 'finetuned', 'peft']:\n",
    "    r = {'model': label}\n",
    "    for split, df in dfs.items():\n",
    "        r[f'{split}_accuracy'] = df[f'correct_{label}'].mean()\n",
    "    results.append(r)\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d584a7a-9157-41af-8d07-9d0684e50b50",
   "metadata": {},
   "source": [
    "## Training works, but generalization not so well.\n",
    "- The original model is not trained to continue the prompt in the friend/enemy schema, so we don't see any correct results (but could still consider token probabilities).\n",
    "- Fine-tuning all parameters gives best the training accuracy.\n",
    "    - Interestingly the fine-tuned model is doing quite badly on the reverse relations.\n",
    "    - There is a slight improvement in test accuracy.\n",
    "- Tuning fewer paramters somewhat improves both train and validation set, but the model doesn't learn about the hold-out set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9a06111f-7a11-47d9-ab2d-2e638eec2355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_peft\n",
       "enemy     33\n",
       "friend    16\n",
       "other      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick sanity test for class imbalance => not quite balanced, but not terrible\n",
    "dfs['test']['predicted_peft'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f98f7b8c-fb8c-4837-9d90-e9b354551e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 10,  1],\n",
       "       [14,  6,  0],\n",
       "       [ 0,  0,  0]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = dfs['test']['chosen'].str.strip().values\n",
    "y_pred = dfs['test']['predicted_peft'].str.strip().values\n",
    "confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb5873-68dd-4466-b445-8b7d6e1f4dab",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "- Not that simple to inject facts into an LLM by naive finetuning.\n",
    "- Although the model memorizes the given sentences, it forgets its original knowledge\n",
    "- Parameter efficient fine tuning may do a little better for simple generalization of the learnt facts.\n",
    "- I didn't check if the model learns the class of the characters (goodies or badies) => if it did it could generalize to some of the test facts.\n",
    "- Of cource, it could be that this model is just too small.\n",
    " \n",
    "### Maybe direct preference optimization is better than fine-tuning on the facts?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f428bef4-cabc-4b8b-9ca1-6784738145d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
